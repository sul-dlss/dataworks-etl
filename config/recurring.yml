# production:
#   periodic_cleanup:
#     class: CleanSoftDeletedRecordsJob
#     queue: background
#     args: [ 1000, { batch_size: 500 } ]
#     schedule: every hour
#   periodic_command:
#     command: "SoftDeletedRecord.due.delete_all"
#     priority: 2
#     schedule: at 5am every day
production:
  # Note that this schedule avoids hitting the same provider concurrently by putting jobs on different days.
  # Also, this places the transfor and load job after the extract jobs (though there isn't absolutely required).
  # Don't forget to create HB checkins for jobs.
  redivis_stanfordphs_etl:
    class: RedivisExtractJob
    args: [{ organization: "StanfordPHS" }]
    schedule: every sunday
  redivis_sul_etl:
    class: RedivisExtractJob
    args: [{ organization: "SUL" }]
    schedule: every monday
  redivis_sdss_etl:
    class: RedivisExtractJob
    args: [{ organization: "SDSS" }]
    schedule: every tuesday
  datacite_affiliation_etl:
    class: DataciteExtractJob
    args: [{ affiliation: "Stanford University" }]
    schedule: every sunday
  open_neuro_etl:
    class: DataciteExtractJob
    args: [{ client_id: "sul.openneuro" }]
    schedule: every monday
  datacite_provider_etl:
    class: DataciteExtractJob
    args: [{ provider_id: "sul" }]
    schedule: every tuesday  
  zenodo_etl:
    class: ZenodoExtractJob
    schedule: every sunday
  local_etl:
    class: LocalExtractJob
    schedule: every sunday
  dryad_etl:
    class: DryadExtractJob
    schedule: every sunday
  icpsr_etl:
    class: SearchworksExtractJob
    args:
      [
        {
          query_label: "ICPSR",
          solr_params:
            {
              q: "Inter-university Consortium for Political and Social Research.",
              search_field: "search_author",
              fq: ["access_facet:Online", "format_main_ssim:Dataset"],
            },
        },
      ]
    schedule: every sunday
  tl:
    class: TransformLoadJob
    schedule: every thursday
